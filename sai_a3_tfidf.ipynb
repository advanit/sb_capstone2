{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sai: spooky author identification\n",
    "## analysis 3: TF-IDF Vectorizer\n",
    "\n",
    "## strategy\n",
    "This will apply TF-IDF vectorizer with the following classifiers:\n",
    "\n",
    "* Multinomial Naive Bayes \n",
    "* Logistic Regression\n",
    "* Random Forest\n",
    "\n",
    "The same practices done with CountVectorizer, will occur here as well.  Therefore, it will create about 6 experiments:\n",
    "\n",
    "* Split, vectorize TF-IDF + MultinomialNB()\n",
    "* Vectorize TF-IDF, split, MultinomialNB()\n",
    "* Split, vectorize TF-IDF + LogisticRegression()\n",
    "* Vectorize TF-IDF, split, LogisticRegression()\n",
    "* Split, vectorize TF-IDF + RandomForestClassifier()\n",
    "* Vectorize TF-IDF, split, RandomForestClassifier()\n",
    "\n",
    "Runtime will be long.  RandomizedSearchCV will be used for all to save time.\n",
    "\n",
    "## code\n",
    "### preliminaries\n",
    "This is the 'de facto' run, where it loads libraries and necessary modules to perform the analysis.  Afterwards, it will read a simple csv file into a dataframe called 'texts.'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn\n",
    "from sklearn.cross_validation import train_test_split             # cross-validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer      # vectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB                     # classifier\n",
    "from sklearn.linear_model import LogisticRegression               # classifier\n",
    "from sklearn.ensemble import RandomForestClassifier               # classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier                 # classifier\n",
    "from sklearn.model_selection import GridSearchCV                  # parameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV            # parameter tuning\n",
    "from sklearn.pipeline import Pipeline                             # pipeline\n",
    "from sklearn import metrics                                       # metrics\n",
    "\n",
    "# other modules\n",
    "from stop_words import get_stop_words\n",
    "from scipy.stats import randint as sp_randint\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "# Read training texts: texts\n",
    "texts = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize + Naive Bayes = Experiment 1 (Split / Vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "X = texts.text\n",
    "y = texts.author\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline_A1 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# parameters\n",
    "parameters_A1 = dict(\n",
    "    tfidf__ngram_range = [(1,1), (1,2), (1,3)],\n",
    "    tfidf__max_df = (0.5, 0.75, 1.0),\n",
    "    nb__alpha = [0.05, 0.1, 1.0, 2.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cv=5 academically proven as best fold, kept n_jobs (jobs running parallel)= 1, output time\n",
    "rand_search_A1 = RandomizedSearchCV(pipeline_A1, \n",
    "                           parameters_A1, \n",
    "                           n_jobs=1, \n",
    "                           cv=5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'nb']\n",
      "parameters:\n",
      "{'nb__alpha': [0.05, 0.1, 1.0, 2.0],\n",
      " 'tfidf__max_df': (0.5, 0.75, 1.0),\n",
      " 'tfidf__ngram_range': [(...), (...), (...)]}\n",
      "CPU times: user 2min 21s, sys: 4.33 s, total: 2min 25s\n",
      "Wall time: 2min 26s\n",
      "Best score: 0.846\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.05\n",
      "\ttfidf__max_df: 0.75\n",
      "\ttfidf__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# fit model for best parameters\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_A1.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_A1, depth=2)\n",
    "\n",
    "%time rand_search_A1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % rand_search_A1.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = rand_search_A1.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters_A1.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate estimator\n",
    "vect_A1 = TfidfVectorizer(binary=True, ngram_range=(1,2), stop_words=None, max_df=0.75)\n",
    "nb_A1 = MultinomialNB(alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit & transform with vectorizer\n",
    "X_train_dtm = vect_A1.fit_transform(X_train)\n",
    "\n",
    "# fit with classifer\n",
    "nb_A1.fit(X_train_dtm, y_train)\n",
    "\n",
    "# predict with classifier\n",
    "y_pred_train = nb_A1.predict(X_train_dtm)\n",
    "\n",
    "# transform with vectorizer, then predict with classifier\n",
    "X_test_dtm = vect_A1.transform(X_test)\n",
    "y_pred_test = nb_A1.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999182783983\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852706843718\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5922    3    3]\n",
      " [   0 4220    0]\n",
      " [   3    3 4530]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1745  184  166]\n",
      " [  79 1128   44]\n",
      " [ 151   97 1301]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       1.00      1.00      1.00      5928\n",
      "        HPL       1.00      1.00      1.00      4220\n",
      "        MWS       1.00      1.00      1.00      4536\n",
      "\n",
      "avg / total       1.00      1.00      1.00     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.88      0.83      0.86      2095\n",
      "        HPL       0.80      0.90      0.85      1251\n",
      "        MWS       0.86      0.84      0.85      1549\n",
      "\n",
      "avg / total       0.86      0.85      0.85      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize + Naive Bayes = Experiment 2 (Vectorize / Split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'nb']\n",
      "parameters:\n",
      "{'nb__alpha': [0.05, 0.1, 1.0, 2.0],\n",
      " 'tfidf__max_df': (0.5, 0.75, 1.0),\n",
      " 'tfidf__ngram_range': [(...), (...), (...)]}\n",
      "CPU times: user 3min 19s, sys: 5.9 s, total: 3min 25s\n",
      "Wall time: 3min 25s\n",
      "Best score: 0.854\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n",
      "\ttfidf__max_df: 1.0\n",
      "\ttfidf__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# fit model for best parameters\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_A1.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_A1, depth=2)\n",
    "\n",
    "%time rand_search_A1.fit(X, y)\n",
    "\n",
    "print(\"Best score: %0.3f\" % rand_search_A1.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = rand_search_A1.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters_A1.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate estimator\n",
    "vect_A2 = TfidfVectorizer(binary=True, ngram_range=(1,2), stop_words=None, max_df=1.0)\n",
    "nb_A2 = MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit with vectorizer\n",
    "vect_A2.fit(X)\n",
    "\n",
    "# transform with vectorizer\n",
    "X_train_dtm = vect_A2.transform(X_train)\n",
    "\n",
    "# fit with classifer\n",
    "nb_A2.fit(X_train_dtm, y_train)\n",
    "\n",
    "# predict with classifier\n",
    "y_pred_train = nb_A2.predict(X_train_dtm)\n",
    "\n",
    "# transform with vectorizer, then predict with classifier\n",
    "X_test_dtm = vect_A2.transform(X_test)\n",
    "y_pred_test = nb_A2.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999046581313\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85393258427\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5923    5    4]\n",
      " [   0 4218    0]\n",
      " [   2    3 4529]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1748  188  157]\n",
      " [  77 1121   43]\n",
      " [ 150  100 1311]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       1.00      1.00      1.00      5932\n",
      "        HPL       1.00      1.00      1.00      4218\n",
      "        MWS       1.00      1.00      1.00      4534\n",
      "\n",
      "avg / total       1.00      1.00      1.00     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.89      0.84      0.86      2093\n",
      "        HPL       0.80      0.90      0.85      1241\n",
      "        MWS       0.87      0.84      0.85      1561\n",
      "\n",
      "avg / total       0.86      0.85      0.85      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize + Logistic Regression = Experiment 1 (Split / Vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pipeline\n",
    "pipeline_B1 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('log_reg', LogisticRegression()),\n",
    "])\n",
    "\n",
    "# parameters\n",
    "parameters_B1 = dict(\n",
    "    tfidf__ngram_range = [(1,1), (1,2), (1,3)],\n",
    "    tfidf__max_df = (0.5, 0.75, 1.0),\n",
    "    log_reg__C = [0.05, 0.1, 1.0, 2.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cv=5 academically proven as best fold, kept n_jobs (jobs running parallel)= 1, output time\n",
    "rand_search_B1 = RandomizedSearchCV(pipeline_B1, \n",
    "                           parameters_B1, \n",
    "                           n_jobs=1, \n",
    "                           cv=5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'log_reg']\n",
      "parameters:\n",
      "{'log_reg__C': [0.05, 0.1, 1.0, 2.0],\n",
      " 'tfidf__max_df': (0.5, 0.75, 1.0),\n",
      " 'tfidf__ngram_range': [(...), (...), (...)]}\n",
      "CPU times: user 4min, sys: 3.72 s, total: 4min 4s\n",
      "Wall time: 2min 32s\n",
      "Best score: 0.795\n",
      "Best parameters set:\n",
      "\tlog_reg__C: 2.0\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# fit model for best parameters\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_B1.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_B1, depth=2)\n",
    "\n",
    "%time rand_search_B1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best score: %0.3f\" % rand_search_B1.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = rand_search_B1.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters_B1.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate estimator\n",
    "vect_B1 = TfidfVectorizer(binary=True, ngram_range=(1,2), stop_words=None, max_df=0.5)\n",
    "logreg_B1 = LogisticRegression(C=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit & transform with vectorizer\n",
    "X_train_dtm = vect_B1.fit_transform(X_train)\n",
    "\n",
    "# fit with classifer\n",
    "logreg_B1.fit(X_train_dtm, y_train)\n",
    "\n",
    "# predict with classifier\n",
    "y_pred_train = logreg_B1.predict(X_train_dtm)\n",
    "\n",
    "# transform with vectorizer, then predict with classifier\n",
    "X_test_dtm = vect_B1.transform(X_test)\n",
    "y_pred_test = logreg_B1.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.983587578317\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812257405516\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5873   66   78]\n",
      " [  20 4142   27]\n",
      " [  32   18 4428]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1673  241  190]\n",
      " [ 135 1070   88]\n",
      " [ 167   98 1233]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.99      0.98      0.98      6017\n",
      "        HPL       0.98      0.99      0.98      4189\n",
      "        MWS       0.98      0.99      0.98      4478\n",
      "\n",
      "avg / total       0.98      0.98      0.98     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.85      0.80      0.82      2104\n",
      "        HPL       0.76      0.83      0.79      1293\n",
      "        MWS       0.82      0.82      0.82      1498\n",
      "\n",
      "avg / total       0.81      0.81      0.81      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize + Logistic Regression = Experiment 2 (Vectorize / Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'log_reg']\n",
      "parameters:\n",
      "{'log_reg__C': [0.05, 0.1, 1.0, 2.0],\n",
      " 'tfidf__max_df': (0.5, 0.75, 1.0),\n",
      " 'tfidf__ngram_range': [(...), (...), (...)]}\n",
      "CPU times: user 5min 25s, sys: 7.69 s, total: 5min 33s\n",
      "Wall time: 3min 39s\n",
      "Best score: 0.819\n",
      "Best parameters set:\n",
      "\tlog_reg__C: 2.0\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# fit model for best parameters\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline_B1.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters_B1, depth=2)\n",
    "\n",
    "%time rand_search_B1.fit(X, y)\n",
    "\n",
    "print(\"Best score: %0.3f\" % rand_search_B1.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = rand_search_B1.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters_B1.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate estimator\n",
    "vect_B2 = TfidfVectorizer(binary=True, ngram_range=(1,1), stop_words=None, max_df=0.5)\n",
    "logreg_B2 = LogisticRegression(C=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit with vectorizer\n",
    "vect_B2.fit(X)\n",
    "\n",
    "# transform with vectorizer\n",
    "X_train_dtm = vect_B2.transform(X_train)\n",
    "\n",
    "# fit with classifer\n",
    "logreg_B2.fit(X_train_dtm, y_train)\n",
    "\n",
    "# predict with classifier\n",
    "y_pred_train = logreg_B2.predict(X_train_dtm)\n",
    "\n",
    "# transform with vectorizer, then predict with classifier\n",
    "X_test_dtm = vect_B2.transform(X_test)\n",
    "y_pred_test = logreg_B2.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.937278670662\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811031664964\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5638  214  249]\n",
      " [ 106 3940   99]\n",
      " [ 181   72 4185]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1699  252  233]\n",
      " [ 128 1081   88]\n",
      " [ 148   76 1190]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.95      0.92      0.94      6101\n",
      "        HPL       0.93      0.95      0.94      4145\n",
      "        MWS       0.92      0.94      0.93      4438\n",
      "\n",
      "avg / total       0.94      0.94      0.94     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.86      0.78      0.82      2184\n",
      "        HPL       0.77      0.83      0.80      1297\n",
      "        MWS       0.79      0.84      0.81      1414\n",
      "\n",
      "avg / total       0.81      0.81      0.81      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_pred_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize + Random Forest = Experiment 1 (Split / Vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning\n",
    "# pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(binary=True, stop_words=None)),\n",
    "    ('rf', RandomForestClassifier(oob_score=True, \n",
    "                                  random_state=1234, \n",
    "                                  warm_start=True,\n",
    "                                  bootstrap=True))\n",
    "])\n",
    "\n",
    "# parameters (please note too many)\n",
    "parameters = dict(\n",
    "    rf__max_features = ['sqrt','log2'],\n",
    "    rf__criterion = [\"gini\", \"entropy\"],\n",
    "    rf__max_depth = [3, None],\n",
    "    rf__min_samples_split = sp_randint(2, 11),\n",
    "    rf__min_samples_leaf =  sp_randint(1, 11),\n",
    "    rf__n_estimators = [10, 25, 50, 75, 100, 125, 150, 175, 200],\n",
    "    vect__max_df = (0.5, 0.75, 1.0),\n",
    "    vect__ngram_range = [(1,1), (1,2), (1,3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cv=5 academically proven as best fold, kept n_jobs (jobs running parallel)= 1, output time\n",
    "rand_search = RandomizedSearchCV(pipeline, \n",
    "                           parameters, \n",
    "                           n_jobs=1, \n",
    "                           cv=5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'rf']\n",
      "parameters:\n",
      "{'rf__criterion': ['gini', 'entropy'],\n",
      " 'rf__max_depth': [3, None],\n",
      " 'rf__max_features': ['sqrt', 'log2'],\n",
      " 'rf__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1121079b0>,\n",
      " 'rf__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x114cbe9b0>,\n",
      " 'rf__n_estimators': [10, 25, 50, 75, 100, 125, 150, 175, 200],\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': [(...), (...), (...)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 52s, sys: 12 s, total: 9min 4s\n",
      "Wall time: 9min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "   ...stimators=10, n_jobs=1, oob_score=True, random_state=1234,\n",
       "            verbose=0, warm_start=True))]),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'rf__max_features': ['sqrt', 'log2'], 'rf__criterion': ['gini', 'entropy'], 'rf__max_depth': [3, None], 'rf__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x114cbe9b0>, 'rf__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1121079b0>, 'rf__n_estimators': [10, 25, 50, 75, 100, 125, 150, 175, 200], 'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model for best parameters\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters, depth=2)\n",
    "\n",
    "%time rand_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.654\n",
      "Best parameters set:\n",
      "\trf__criterion: 'entropy'\n",
      "\trf__max_depth: None\n",
      "\trf__max_features: 'sqrt'\n",
      "\trf__min_samples_leaf: 6\n",
      "\trf__min_samples_split: 4\n",
      "\trf__n_estimators: 175\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % rand_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = rand_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "vect_C1 = TfidfVectorizer(max_df=0.5, ngram_range=(1,2))\n",
    "\n",
    "# random forest classifier; please note this is NOT including parameters\n",
    "rf_C1 = RandomForestClassifier(oob_score=True, \n",
    "                               warm_start=True,\n",
    "                               max_depth=None,\n",
    "                               criterion='entropy',\n",
    "                               max_features='sqrt',\n",
    "                               min_samples_leaf=6,\n",
    "                               min_samples_split=4,\n",
    "                               n_estimators=175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit & transform with vectorizer\n",
    "X_train_dtm = vect_C1.fit_transform(X_train)\n",
    "\n",
    "# transform test set\n",
    "X_test_dtm = vect_C1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.49 s, sys: 268 ms, total: 7.75 s\n",
      "Wall time: 7.76 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=6,\n",
       "            min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=175, n_jobs=1, oob_score=True, random_state=None,\n",
       "            verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit with classifer\n",
    "%time rf_C1.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with training class\n",
    "y_train_pred = rf_C1.predict(X_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with testing class\n",
    "y_test_pred = rf_C1.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74135113048215751"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, train\n",
    "metrics.accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66864147088866188"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5699,   70,  156],\n",
       "       [1662, 2473,   91],\n",
       "       [1671,  148, 2714]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix, train\n",
    "metrics.confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1855,   33,   87],\n",
       "       [ 735,  619,   55],\n",
       "       [ 653,   59,  799]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.63      0.96      0.76      5925\n",
      "        HPL       0.92      0.59      0.72      4226\n",
      "        MWS       0.92      0.60      0.72      4533\n",
      "\n",
      "avg / total       0.80      0.74      0.74     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report, train\n",
    "print(metrics.classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.57      0.94      0.71      1975\n",
      "        HPL       0.87      0.44      0.58      1409\n",
      "        MWS       0.85      0.53      0.65      1511\n",
      "\n",
      "avg / total       0.74      0.67      0.66      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report, test\n",
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679310814492\n"
     ]
    }
   ],
   "source": [
    "# OOB Score\n",
    "print(rf_C1.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize + Random Forest = Experiment 2 (Vectorize / Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'rf']\n",
      "parameters:\n",
      "{'rf__criterion': ['gini', 'entropy'],\n",
      " 'rf__max_depth': [3, None],\n",
      " 'rf__max_features': ['sqrt', 'log2'],\n",
      " 'rf__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1121079b0>,\n",
      " 'rf__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x114cbe9b0>,\n",
      " 'rf__n_estimators': [10, 25, 50, 75, 100, 125, 150, 175, 200],\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': [(...), (...), (...)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 31s, sys: 14.1 s, total: 8min 45s\n",
      "Wall time: 8min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "   ...stimators=10, n_jobs=1, oob_score=True, random_state=1234,\n",
       "            verbose=0, warm_start=True))]),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'rf__max_features': ['sqrt', 'log2'], 'rf__criterion': ['gini', 'entropy'], 'rf__max_depth': [3, None], 'rf__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x114cbe9b0>, 'rf__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1121079b0>, 'rf__n_estimators': [10, 25, 50, 75, 100, 125, 150, 175, 200], 'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model for best parameters\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters, depth=2)\n",
    "\n",
    "%time rand_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.666\n",
      "Best parameters set:\n",
      "\trf__criterion: 'gini'\n",
      "\trf__max_depth: None\n",
      "\trf__max_features: 'sqrt'\n",
      "\trf__min_samples_leaf: 1\n",
      "\trf__min_samples_split: 7\n",
      "\trf__n_estimators: 75\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % rand_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = rand_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "vect_C2 = TfidfVectorizer(max_df=1.0, ngram_range=(1,2))\n",
    "\n",
    "# random forest classifier; please note this is NOT including parameters\n",
    "rf_C2 = RandomForestClassifier(oob_score=True, \n",
    "                               warm_start=True,\n",
    "                               max_depth=None,\n",
    "                               criterion='gini',\n",
    "                               max_features='sqrt',\n",
    "                               min_samples_leaf=1,\n",
    "                               min_samples_split=7,\n",
    "                               n_estimators=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with vectorizer\n",
    "vect_C2.fit(X)\n",
    "\n",
    "# transform train set\n",
    "X_train_dtm = vect_C2.transform(X_train)\n",
    "\n",
    "# transform test set\n",
    "X_test_dtm = vect_C2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:303: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 928 ms, sys: 78.9 ms, total: 1.01 s\n",
      "Wall time: 1.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=7, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=75, n_jobs=1, oob_score=True, random_state=None,\n",
       "            verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit with training class\n",
    "%time rf_C2.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with training class\n",
    "y_train_pred = rf_C2.predict(X_train_dtm)\n",
    "\n",
    "# predict with testing class\n",
    "y_test_pred = rf_C2.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, train\n",
    "metrics.accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67150153217568953"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5925,    0,    0],\n",
       "       [   0, 4226,    0],\n",
       "       [   0,    0, 4533]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "metrics.confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1795,   72,  108],\n",
       "       [ 661,  694,   54],\n",
       "       [ 607,  106,  798]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       1.00      1.00      1.00      5925\n",
      "        HPL       1.00      1.00      1.00      4226\n",
      "        MWS       1.00      1.00      1.00      4533\n",
      "\n",
      "avg / total       1.00      1.00      1.00     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "print(metrics.classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.59      0.91      0.71      1975\n",
      "        HPL       0.80      0.49      0.61      1409\n",
      "        MWS       0.83      0.53      0.65      1511\n",
      "\n",
      "avg / total       0.72      0.67      0.66      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66391991283\n"
     ]
    }
   ],
   "source": [
    "# OOB Score\n",
    "print(rf_C2.oob_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
