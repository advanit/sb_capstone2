{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sai: spooky author identification\n",
    "## analysis 3: random forest\n",
    "\n",
    "## discussion\n",
    "In the previous code, the author identification analysis was performed with Naive Bayes Classification and Logistic Regression.  This used CountVectorizer to identify the author based on the text prose and split the training set into two areas: split and vectorize, or vectorize and split.  The later proved productive; therefore, this experiment will use random forests with hyperparameter tuning with CountVectorizer to obtain better predictions.  \n",
    "\n",
    "\n",
    "## code\n",
    "### preliminaries\n",
    "Import libraries and modules.  Read csv file.  And show what's in it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn\n",
    "from sklearn.cross_validation import train_test_split             # cross-validation\n",
    "from sklearn.feature_extraction.text import CountVectorizer       # vectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier               # classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier                 # classifier\n",
    "from sklearn.model_selection import GridSearchCV                  # parameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV            # parameter tuning\n",
    "from sklearn.pipeline import Pipeline                             # pipeline\n",
    "from sklearn import metrics                                       # metrics\n",
    "\n",
    "# other modules\n",
    "from stop_words import get_stop_words\n",
    "from scipy.stats import randint as sp_randint\n",
    "import string\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Read training texts: texts\n",
    "texts = pd.read_csv('train.csv')\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Trial (no hyperparameter tuning)\n",
    "#### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose X and y\n",
    "X = texts.text\n",
    "y = texts.author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document-term matrix\n",
    "X_dtm = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform training case\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "\n",
    "# transform test case\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest classifier; please note this is NOT including parameters\n",
    "rf = RandomForestClassifier(oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.2 s, sys: 31.1 ms, total: 5.23 s\n",
      "Wall time: 5.23 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=True, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "%time rf.fit(X_dtm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with training class\n",
    "y_train_pred = rf.predict(X_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with testing class\n",
    "y_test_pred = rf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.610858572961\n"
     ]
    }
   ],
   "source": [
    "# oob score\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98896758376464178"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, train\n",
    "metrics.accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9893769152196118"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5942,   10,   12],\n",
       "       [  62, 4118,   11],\n",
       "       [  50,   17, 4462]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix, train\n",
    "metrics.confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1923,    8,    5],\n",
       "       [  17, 1426,    1],\n",
       "       [  13,    8, 1494]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.98      1.00      0.99      5964\n",
      "        HPL       0.99      0.98      0.99      4191\n",
      "        MWS       0.99      0.99      0.99      4529\n",
      "\n",
      "avg / total       0.99      0.99      0.99     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report, train\n",
    "print(metrics.classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.98      0.99      0.99      1936\n",
      "        HPL       0.99      0.99      0.99      1444\n",
      "        MWS       1.00      0.99      0.99      1515\n",
      "\n",
      "avg / total       0.99      0.99      0.99      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report, test\n",
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "X_tokens = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "tokens_rf = pd.DataFrame({'token':X_tokens, 'metric':rf.feature_importances_}).set_index('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "raymond    0.009076\n",
      "upon       0.007015\n",
      "though     0.006953\n",
      "perdita    0.005213\n",
      "love       0.005178\n",
      "old        0.004164\n",
      "adrian     0.003842\n",
      "father     0.003819\n",
      "life       0.003697\n",
      "towards    0.002850\n",
      "Name: metric, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokens_rf['metric'].nlargest(10),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updated parameters \n",
    "This will use the code from sci-kit learn developers to estimate the OOB (out of bag) error and the best feature handler for this dataset.  Here, the n_estimators is 100; n_estimators  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model =  RandomForestClassifier(n_estimators = 100, oob_score = True, warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.8 s, sys: 322 ms, total: 52.1 s\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%time model.fit(X_dtm, y)\n",
    "y_train_pred = model.predict(X_train_dtm)\n",
    "y_test_pred = model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5964    0    0]\n",
      " [   1 4190    0]\n",
      " [   1    0 4528]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       1.00      1.00      1.00      5964\n",
      "        HPL       1.00      1.00      1.00      4191\n",
      "        MWS       1.00      1.00      1.00      4529\n",
      "\n",
      "avg / total       1.00      1.00      1.00     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999795709908\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.610858572961\n"
     ]
    }
   ],
   "source": [
    "# oob score\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1936    0    0]\n",
      " [   0 1444    0]\n",
      " [   1    0 1514]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       1.00      1.00      1.00      1936\n",
      "        HPL       1.00      1.00      1.00      1444\n",
      "        MWS       1.00      1.00      1.00      1515\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocabulary (same as first trial)\n",
    "X_tokens = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "tokens_rf = pd.DataFrame({'token':X_tokens, 'metric':model.feature_importances_}).set_index('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "raymond    0.008629\n",
      "upon       0.008079\n",
      "perdita    0.005141\n",
      "love       0.004937\n",
      "though     0.004735\n",
      "adrian     0.004516\n",
      "old        0.004420\n",
      "father     0.003796\n",
      "west       0.003255\n",
      "towards    0.003008\n",
      "Name: metric, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokens_rf['metric'].nlargest(10),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter tuning\n",
    "### discussion\n",
    "Random Forests may be the best estimator; however, tuning them is not!  Below is a simple RandomForestClassifier model:\n",
    "\n",
    "`\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "            verbose=0, warm_start=False)\n",
    "`\n",
    "\n",
    "\n",
    "There are five different parameters to evaluate for a better score.  And with computational time an issue, it is best to break down each parameter and promote one with the best estimate parameters.  \n",
    "\n",
    "* max_features=`['sqrt','none','log2']`\n",
    "* criterion=`['gini','entropy']`\n",
    "* n_estimators=`[50, 100, 150, 200]`\n",
    "* min_samples_leaf=`[50,100,150]`\n",
    "\n",
    "Other parameters will make it easy to train; this includes:\n",
    "\n",
    "* random_state=42\n",
    "* oob_score=True \n",
    "* warm_start=True (use previous build to build new set)\n",
    "\n",
    "\n",
    "### 1 - `n_estimators` \n",
    "This will use the code from sci-kit learn developers to estimate the OOB (out of bag) error and the best feature handler for this dataset.  Here, the n_estimators is 10; n_estimators will increase every 10 steps per iteration so the final n will be 410 (40 * 10 + 10).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "oob_error = []\n",
    "growing_rf = RandomForestClassifier(n_estimators=10, \n",
    "                                    n_jobs=-1,  \n",
    "                                    warm_start=True,\n",
    "                                    oob_score=True,\n",
    "                                    random_state=1514)\n",
    "\n",
    "for i in range(40):\n",
    "    growing_rf.fit(X_train_dtm, y_train)\n",
    "    errors.append(metrics.log_loss(y_test, growing_rf.predict_proba(X_test_dtm)))\n",
    "    oob_error.append([growing_rf.n_estimators, 1 - growing_rf.oob_score_])\n",
    "    growing_rf.n_estimators += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for i in range(len(oob_error)):\n",
    "    y.append(oob_error[i][1])\n",
    "    x.append(oob_error[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(20,5))\n",
    "_ = plt.plot(x,y, '-r')\n",
    "_ = plt.title('OOB Error by Time', size=14)\n",
    "_ = plt.xlabel('n_estimators', size=14)\n",
    "_ = plt.ylabel('OOB Error', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, within the first 5 iterations (where n_estimators = 10 to 60), the error goes down significantly but afterwards, it hovers around 0.77-0.79.  So it steadied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2 - `n_estimators`  +  `max_features`\n",
    "This will use the same code but will also use `max_features`.  This comes from the following link: http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html#sphx-glr-auto-examples-ensemble-plot-ensemble-oob-py.  This is essential to see how one feature does when another feature gets tweaked.  And it will determine a better choice for `max_features`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Setting the `warm_start` construction parameter to `True` disables\n",
    "# support for parallelized ensembles but is necessary for tracking the OOB\n",
    "# error trajectory during training.\n",
    "\n",
    "ensemble_clfs = [\n",
    "    (\"RandomForestClassifier, max_features='sqrt'\",\n",
    "        RandomForestClassifier(warm_start=True, \n",
    "                               oob_score=True,\n",
    "                               max_features=\"sqrt\",\n",
    "                               n_estimators=10,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features='log2'\",\n",
    "        RandomForestClassifier(warm_start=True, \n",
    "                               max_features='log2',\n",
    "                               n_estimators=10,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE)),\n",
    "    (\"RandomForestClassifier, max_features=None\",\n",
    "        RandomForestClassifier(warm_start=True, \n",
    "                               max_features=None,\n",
    "                               n_estimators=10,\n",
    "                               oob_score=True,\n",
    "                               random_state=RANDOM_STATE))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\n",
    "error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "oob_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n",
    "\n",
    "for label, clf in ensemble_clfs:\n",
    "    for i in range(40):\n",
    "        clf.fit(X_train_dtm, y_train)\n",
    "        \n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        log_loss_error = metrics.log_loss(y_test, clf.predict_proba(X_test_dtm))\n",
    "        \n",
    "        # Enter error rate details\n",
    "        error_rate[label].append((i, clf.n_estimators, log_loss_error))\n",
    "        oob_rate[label].append((i, clf.n_estimators, oob_error))\n",
    "        \n",
    "        # Update n_estimators \n",
    "        clf.n_estimators += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys, zs = zip(*clf_err)\n",
    "    plt.plot(ys, zs, label=label)\n",
    "\n",
    "plt.xlabel(\"n_estimators\", fontsize=14)\n",
    "plt.ylabel(\"log loss rate\", fontsize=14)\n",
    "plt.title('Log Loss vs n_estimators by max_feature', size=14)\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, the first five estimators start with high errors.  However, the error goes down.  However, with NO maximum features, it steadied around 0.38-0.39.  With the other two maximum features, the 'sqrt' random forest had a slightly higher error than the 'log2' random forest but the 'log2' overtook it, having a smaller error over the long term.  This would continue for another 50 n_estimators.  \n",
    "\n",
    "Working with n_estimators, the log2 random forest does well with this training set than the sqrt random forest and especially the random forest where there are no maximum features.  Even in a quick grid search, `sqrt` did the best with the training set although the best values were not present.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the \"log loss\" vs. \"n_estimators\" plot.\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "for label, clf_err in oob_rate.items():\n",
    "    xs, ys, zs  = zip(*clf_err)\n",
    "    plt.plot(ys, zs, label=label)\n",
    "\n",
    "plt.xlabel(\"n_estimators\", fontsize=14)\n",
    "plt.ylabel(\"OOB error rate\", fontsize=14)\n",
    "plt.title('OOB Error vs n_estimators by max_feature', size=14)\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the same evaluation metric, log_loss, the 'None' random forest has the same curvature as above.  However, the two have steady log-loss error rates below 0.80.  'log2', which did well in the OOB error curve, had a larger error by 0.01-0.02 against 'sqrt', as to why the pipeline preferred 'sqrt' as the best.  But also, it doesn't seem that one parameter is influencing the other.  While this is a simple choice, it will remain the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - `min_samples_leaf`\n",
    "This will now test the minimum sample leafs using the information obtained above.  Seeing that the n_estimators perform well after 50 and that the 'log2' random forest and the 'sqrt' random forest did better than having no maximum features, it would be best to continue on this route.  Hypothetically if having 15 possibilities per feature to check, then with n features, it would recur 15 * n, and this is not including cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_leaf_options = list(range(1,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leaf_error = []\n",
    "oob_rate_leaf = []\n",
    "\n",
    "for leaf_size in sample_leaf_options:\n",
    "    clf = RandomForestClassifier(n_estimators = 100, \n",
    "                                oob_score = True, \n",
    "                                warm_start = True,\n",
    "                                n_jobs = -1,\n",
    "                                random_state = RANDOM_STATE, \n",
    "                                max_features = \"log2\", \n",
    "                                min_samples_leaf = leaf_size)\n",
    "    \n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    \n",
    "    # OOB and log-loss error\n",
    "    \n",
    "    log_loss_error = metrics.log_loss(y_test, clf.predict_proba(X_test_dtm))\n",
    "    \n",
    "        \n",
    "    # Enter error rate details\n",
    "    leaf_error.append((leaf_size, log_loss_error))\n",
    "    oob_rate_leaf.append((leaf_size, clf.oob_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "for i in range(len(oob_rate_leaf)):\n",
    "    y.append(oob_rate_leaf[i][1])\n",
    "    x.append(oob_rate_leaf[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(20,5))\n",
    "_ = plt.plot(x,y, '-r')\n",
    "_ = plt.title('OOB Error by Time', size=14)\n",
    "_ = plt.xlabel('min_samples_leaf', size=14)\n",
    "_ = plt.ylabel('OOB Error', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default leaf sample size (= 1) seems to be the best choice.  As the minimal sample increases so does the leaf error (the log-loss error when changing the number).  Also, after the ninth iteration, the error will remain stabilized.  It will still compute but it will stabilize.  Thus the min_samples_leaf is best left alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Other Tuning Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning\n",
    "# pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(binary=True, stop_words=None)),\n",
    "    ('rf', RandomForestClassifier(oob_score=True, \n",
    "                                  random_state=RANDOM_STATE, \n",
    "                                  warm_start=True,\n",
    "                                 bootstrap=True))\n",
    "])\n",
    "\n",
    "# parameters (please note too many)\n",
    "parameters = dict(\n",
    "    rf__max_features = ['sqrt','log2'],\n",
    "    rf__criterion = [\"gini\", \"entropy\"],\n",
    "    rf__max_depth = [3, None],\n",
    "    rf__min_samples_split = sp_randint(2, 11),\n",
    "    rf__min_samples_leaf =  sp_randint(1, 11),\n",
    "    rf__n_estimators = [10, 25, 50, 75, 100, 125, 150, 175, 200],\n",
    "    vect__max_df = (0.5, 0.75, 1.0),\n",
    "    vect__ngram_range = [(1,1), (1,2), (2,2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cv=5 academically proven as best fold, kept n_jobs (jobs running parallel)= 1, output time\n",
    "rand_search = RandomizedSearchCV(pipeline, \n",
    "                           parameters, \n",
    "                           n_jobs=1, \n",
    "                           cv=5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'rf']\n",
      "parameters:\n",
      "{'rf__criterion': ['gini', 'entropy'],\n",
      " 'rf__max_depth': [3, None],\n",
      " 'rf__max_features': ['sqrt', 'log2'],\n",
      " 'rf__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x10f88e2b0>,\n",
      " 'rf__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x10f8690f0>,\n",
      " 'rf__n_estimators': [10, 25, 50, 75, 100, 125, 150, 175, 200],\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': [(...), (...), (...)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 7.84 s, total: 2min 55s\n",
      "Wall time: 2min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...estimators=10, n_jobs=1, oob_score=True, random_state=123,\n",
       "            verbose=0, warm_start=True))]),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'rf__max_features': ['sqrt', 'log2'], 'rf__criterion': ['gini', 'entropy'], 'rf__max_depth': [3, None], 'rf__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x10f8690f0>, 'rf__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x10f88e2b0>, 'rf__n_estimators': [10, 25, 50, 75, 100, 125, 150, 175, 200], 'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model for best parameters\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters, depth=2)\n",
    "\n",
    "%time rand_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.678\n",
      "Best parameters set:\n",
      "\trf__criterion: 'entropy'\n",
      "\trf__max_depth: None\n",
      "\trf__max_features: 'sqrt'\n",
      "\trf__min_samples_leaf: 4\n",
      "\trf__min_samples_split: 5\n",
      "\trf__n_estimators: 100\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % rand_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = rand_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'rf']\n",
      "parameters:\n",
      "{'rf__criterion': ['gini', 'entropy'],\n",
      " 'rf__max_depth': [3, None],\n",
      " 'rf__max_features': ['sqrt', 'log2'],\n",
      " 'rf__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x10f88e2b0>,\n",
      " 'rf__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x10f8690f0>,\n",
      " 'rf__n_estimators': [10, 25, 50, 75, 100, 125, 150, 175, 200],\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': [(...), (...), (...)]}\n",
      "CPU times: user 10min 30s, sys: 24.2 s, total: 10min 54s\n",
      "Wall time: 10min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...estimators=10, n_jobs=1, oob_score=True, random_state=123,\n",
       "            verbose=0, warm_start=True))]),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'rf__max_features': ['sqrt', 'log2'], 'rf__criterion': ['gini', 'entropy'], 'rf__max_depth': [3, None], 'rf__min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x10f8690f0>, 'rf__min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x10f88e2b0>, 'rf__n_estimators': [10, 25, 50, 75, 100, 125, 150, 175, 200], 'vect__max_df': (0.5, 0.75, 1.0), 'vect__ngram_range': [(1, 1), (1, 2), (2, 2)]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model for best parameters\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters, depth=2)\n",
    "\n",
    "%time rand_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.663\n",
      "Best parameters set:\n",
      "\trf__criterion: 'entropy'\n",
      "\trf__max_depth: None\n",
      "\trf__max_features: 'sqrt'\n",
      "\trf__min_samples_leaf: 8\n",
      "\trf__min_samples_split: 5\n",
      "\trf__n_estimators: 100\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best score: %0.3f\" % rand_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "\n",
    "best_parameters = rand_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment 1 - split / vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "        \n",
    "vect_rf1 = CountVectorizer(max_df=0.5, \n",
    "                          ngram_range=(1,2),\n",
    "                          stop_words=stop_word)\n",
    "\n",
    "# random forest classifier; please note this is NOT including parameters\n",
    "rf3 = RandomForestClassifier(oob_score = True,\n",
    "                            criterion = 'entropy',\n",
    "                            max_features = 'sqrt',\n",
    "                            min_samples_leaf = 4,\n",
    "                            min_samples_split = 5,\n",
    "                            n_estimators = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:439: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:444: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "# Fit and transform\n",
    "vect_rf1.fit(X_train)\n",
    "X_train_dtm = vect_rf1.transform(X_train)\n",
    "X_test_dtm = vect_rf1.transform(X_test)\n",
    "\n",
    "# Fit and predict\n",
    "rf3.fit(X_train_dtm, y_train)\n",
    "y_train_pred = rf3.predict(X_train_dtm)\n",
    "y_test_pred = rf3.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.583492236448\n"
     ]
    }
   ],
   "source": [
    "# oob score\n",
    "print(rf3.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66557711950970377"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5193,  403,  329],\n",
       "       [1299, 2673,  254],\n",
       "       [1453,  367, 2713]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix, train\n",
    "metrics.confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1658,  173,  144],\n",
       "       [ 519,  780,  110],\n",
       "       [ 530,  161,  820]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.65      0.88      0.75      5925\n",
      "        HPL       0.78      0.63      0.70      4226\n",
      "        MWS       0.82      0.60      0.69      4533\n",
      "\n",
      "avg / total       0.74      0.72      0.72     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report, train\n",
    "print(metrics.classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       0.61      0.84      0.71      1975\n",
      "        HPL       0.70      0.55      0.62      1409\n",
      "        MWS       0.76      0.54      0.63      1511\n",
      "\n",
      "avg / total       0.68      0.67      0.66      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report, test\n",
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "X_tokens = vect_rf1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "tokens_rf = pd.DataFrame({'token':X_tokens, 'metric':rf3.feature_importances_}).set_index('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "raymond    0.015360\n",
      "father     0.013091\n",
      "windsor    0.012269\n",
      "though     0.010241\n",
      "thus       0.009343\n",
      "upon       0.008020\n",
      "adrian     0.007948\n",
      "idris      0.006922\n",
      "thing      0.006913\n",
      "sister     0.006606\n",
      "Name: metric, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokens_rf['metric'].nlargest(10),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## experiment 2 - vectorize / split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "vect_rf = CountVectorizer(max_df=1.0, \n",
    "                          ngram_range=(1,2),\n",
    "                          stop_words=stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn the 'vocabulary' of the training data (occurs in-place)\n",
    "X_dtm = vect_rf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform \n",
    "X_train_dtm = vect_rf.transform(X_train)\n",
    "X_test_dtm = vect_rf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest classifier; please note this is NOT including parameters\n",
    "rf = RandomForestClassifier(oob_score=True, \n",
    "                            warm_start=True,\n",
    "                            max_features='log2',\n",
    "                            min_samples_split=4,\n",
    "                            n_estimators=175\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 50s, sys: 1.34 s, total: 3min 51s\n",
      "Wall time: 3min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=175, n_jobs=1, oob_score=True, random_state=None,\n",
       "            verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "%time rf.fit(X_dtm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with training class\n",
    "y_train_pred = rf.predict(X_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with testing class\n",
    "y_test_pred = rf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for X_test_dtm\n",
    "y_pred_prob = rf.predict_proba(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999591391991283"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, train\n",
    "metrics.accuracy_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71341743705\n"
     ]
    }
   ],
   "source": [
    "# oob score\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99959141981613897"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score, test\n",
    "metrics.accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5925,    0,    0],\n",
       "       [   5, 4221,    0],\n",
       "       [   1,    0, 4532]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix, train\n",
    "metrics.confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1975,    0,    0],\n",
       "       [   1, 1408,    0],\n",
       "       [   1,    0, 1510]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix\n",
    "metrics.confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       1.00      1.00      1.00      5925\n",
      "        HPL       1.00      1.00      1.00      4226\n",
      "        MWS       1.00      1.00      1.00      4533\n",
      "\n",
      "avg / total       1.00      1.00      1.00     14684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report, train\n",
    "print(metrics.classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        EAP       1.00      1.00      1.00      1975\n",
      "        HPL       1.00      1.00      1.00      1409\n",
      "        MWS       1.00      1.00      1.00      1511\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report, test\n",
    "print(metrics.classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "X_tokens = vect_rf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature importance\n",
    "tokens_rf = pd.DataFrame({'token':X_tokens, 'metric':rf.feature_importances_}).set_index('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\n",
      "upon       0.001877\n",
      "raymond    0.001590\n",
      "love       0.001330\n",
      "perdita    0.001265\n",
      "adrian     0.001253\n",
      "father     0.000988\n",
      "though     0.000964\n",
      "old        0.000926\n",
      "heart      0.000900\n",
      "life       0.000838\n",
      "Name: metric, dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokens_rf['metric'].nlargest(10),'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
